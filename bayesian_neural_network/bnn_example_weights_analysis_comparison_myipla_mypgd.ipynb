{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description:** Analyse standard deviation of the posterior of the weights when training the BNN using MYIPLA and MYPGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Load modules.\n",
    "\n",
    "# Numpy and JAX for computations.\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import torch\n",
    "\n",
    "# Pyplot for plots.\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load and curate the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-29 20:08:58.904174: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-29 20:08:58.904203: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-29 20:08:58.904956: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-29 20:08:59.568101: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.1.0) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "#@title Load, subsample, and normalize MNIST dataset.\n",
    "\n",
    "# Load dataset:\n",
    "from keras.datasets import mnist\n",
    "(images, labels), _ = mnist.load_data()\n",
    "images = np.array(images).astype(float)\n",
    "labels = np.array(labels).astype(int)\n",
    "\n",
    "# Keep only datapoints with labels 4 and 9:\n",
    "indices = (labels == 4) | (labels == 9)\n",
    "labels = labels[indices]\n",
    "images = images[indices, :, :]\n",
    "\n",
    "# Relabel as 4 as 0 and 9 as 1:\n",
    "for n in range(labels.size):\n",
    "    if labels[n] == 4:\n",
    "        labels[n] = 0\n",
    "    else:\n",
    "        labels[n] = 1\n",
    "\n",
    "# Sub-sample 1000 images:\n",
    "from sklearn.model_selection import train_test_split\n",
    "images, _, labels, _ = train_test_split(images, labels, train_size=1000,\n",
    "                                        random_state=0)\n",
    "\n",
    "# Normalize non-zero entries (pixels across whole dataset) so that they have mean zero \n",
    "# and unit standard across the dataset:\n",
    "i = images.std(0) != 0\n",
    "images[:, i] = (images[:, i] - images[:, i].mean(0))/images[:, i].std(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then implement the algorithms. They take in the following inputs:\n",
    "\n",
    "*   itrain : training set labels,\n",
    "*   itrain : training set images,\n",
    "*   itest : test set labels,\n",
    "*   itest : test set images,\n",
    "*   h : step-size,\n",
    "*   K : number of steps,\n",
    "*   N : number of particles,\n",
    "*   a : 1-dimensional vector with initial alpha guess,\n",
    "*   b : 1-dimensional vector with initial beta guess,\n",
    "*   w : Dw x N matrix storing the input layer weights of the initial particle cloud,\n",
    "*   v : Dv x N matrix storing the output layer weights of the initial particle cloud.\n",
    "*   gamma: Smoothing parameter of the Moreau-Yosida envelope.\n",
    "\n",
    "They return the following outputs:\n",
    "\n",
    "*   a : K-dimensional vector of alpha estimates,\n",
    "*   b : K-dimensional vector of beta estimates,\n",
    "*   w : Dw x N matrix storing the input layer weights of the final particle cloud,\n",
    "*   v : Dv x N matrix storing the output layer weights of the final particle cloud,\n",
    "*   lppd : log pointwise predictive density (LPPD) as a function of k,\n",
    "*   error : test error as a function of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithms\n",
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))  \n",
    "sys.path.append(project_root)\n",
    "\n",
    "from bayesian_neural_network.algorithms import my_ipla_bnn, my_pgd_bnn\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "os.chdir(project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run the algorithms using an 80/20 training/test split of the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MYIPLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:22<00:00, 11.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# Split data into 80/20 training and testing sets:\n",
    "itrain, itest, ltrain, ltest = train_test_split(images, labels, test_size=0.2,\n",
    "                                                random_state=0)\n",
    "\n",
    "# Set approximation parameters:\n",
    "h = 5e-2 # Step-size. \n",
    "K = 250  # Number of steps.\n",
    "N = 50  # Number of particles.\n",
    "gamma = 0.5 # Smoothing parameter\n",
    "\n",
    "# Initialize parameter estimates:\n",
    "a0 = np.array([1])  # Alpha.\n",
    "b0 = np.array([1])  # Beta.\n",
    "\n",
    "# Initialize particle cloud by sampling prior:'\n",
    "w0 = np.exp(a0)*np.random.normal(0, 1, (40, 28**2, N))  # Input layer weights.\n",
    "v0 = np.exp(b0)*np.random.normal(0, 1, (2, 40, N))  # Output layer weights.\n",
    "\n",
    "# Run algorithms:\n",
    "a_pgd, b_pgd, w_pgd, v_pgd, lppd_pgd, error_pgd = my_ipla_bnn(ltrain, itrain, ltest, \n",
    "                                                      itest, h, K, a0, b0, w0, \n",
    "                                                      v0, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.10660859942436218 0.019999999552965164\n"
     ]
    }
   ],
   "source": [
    "print(lppd_pgd[-1], error_pgd[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviation of a one of the particles X (describing the weights of the BNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2699724320206114"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array(w_pgd[:, :, -1].ravel().tolist())\n",
    "np.std(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MYPGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:20<00:00, 12.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# Split data into 80/20 training and testing sets:\n",
    "itrain, itest, ltrain, ltest = train_test_split(images, labels, test_size=0.2,\n",
    "                                                random_state=0)\n",
    "\n",
    "# Set approximation parameters:\n",
    "h = 5e-2 # Step-size. \n",
    "K = 250  # Number of steps.\n",
    "N = 50  # Number of particles.\n",
    "gamma = 0.35 # Smoothing parameter\n",
    "\n",
    "# Initialize parameter estimates:\n",
    "a0 = np.array([1])  # Alpha.\n",
    "b0 = np.array([1])  # Beta.\n",
    "\n",
    "# Initialize particle cloud by sampling prior:'\n",
    "w0 = np.exp(a0)*np.random.normal(0, 1, (40, 28**2, N))  # Input layer weights.\n",
    "v0 = np.exp(b0)*np.random.normal(0, 1, (2, 40, N))  # Output layer weights.\n",
    "\n",
    "# Run algorithms:\n",
    "a_pgd, b_pgd, w_pgd, v_pgd, lppd_pgd, error_pgd = p_pgd_bnn(ltrain, itrain, ltest, \n",
    "                                                      itest, h, K, a0, b0, w0, \n",
    "                                                      v0, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.10235802084207535 0.014999999664723873\n"
     ]
    }
   ],
   "source": [
    "print(lppd_pgd[-1], error_pgd[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviation of a one of the particles X (describing the weights of the BNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0202537446947995"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array(w_pgd[:, :, -1].ravel().tolist())\n",
    "np.std(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
